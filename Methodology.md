Methodology for Scoring Compound V2 Wallets1. Introduction1.1. Project ObjectiveThis document outlines a comprehensive methodology for developing a machine learning-driven credit scoring system for wallets interacting with the Compound V2 protocol. Commissioned by Zeru Finance as part of their initiative to build an AI-powered decentralized credit scoring system, the primary goal is to assign a numerical score between 0 and 100 to each wallet. This score will serve as an indicator of the wallet's historical behavior within the Compound V2 ecosystem. Higher scores are intended to represent reliable and responsible protocol usage, characterized by prudent borrowing, consistent participation, and adherence to sound risk management principles. Conversely, lower scores aim to identify wallets exhibiting behaviors deemed risky, potentially automated (bot-like), or exploitative. The foundation for this scoring system will be solely the raw, transaction-level data provided, reflecting historical interactions such as deposits (minting cTokens), withdrawals (redeeming cTokens), borrowing, repayments, and liquidations.1.2. The Challenge of Unsupervised ScoringA core challenge inherent in this task is the absence of pre-defined labels or ground truth categorizations for "good" or "bad" wallet behavior [User Query]. Unlike traditional credit scoring which often relies on historical default data, this project requires deriving behavioral assessments directly from interaction patterns within the constraints of the provided dataset. Consequently, the methodology must be grounded in first principles of decentralized finance (DeFi) lending risk, the specific mechanics of the Compound V2 protocol, and data-driven pattern discovery. The development process is strictly limited to the provided raw transaction logs, explicitly excluding the use of external labeled datasets, pre-trained models, third-party scoring systems, or proprietary Zeru Finance code or schema definitions [User Query]. This constraint necessitates an approach that defines and quantifies behavioral quality intrinsically from the observed data.1.3. Scope and DeliverablesThe scope of this methodology encompasses the end-to-end process required to design the scoring system, starting from data selection and preparation through feature engineering, model selection, scoring system design, and concluding with a validation strategy. The focus remains firmly on the plan and methodology, not the implementation code itself. The anticipated deliverables resulting from the execution of this plan include:
Methodology Document: This current document detailing the comprehensive plan.
Code Submission: A script or notebook executing the plan, including data loading, processing, feature extraction, and score generation.
CSV Output: A file containing the wallet addresses and their corresponding scores (0-100) for the top 1,000 wallets, sorted from highest to lowest score.
Wallet Analysis: A concise one-page document analyzing the transaction patterns of representative high-scoring and low-scoring wallets to provide qualitative validation for the scoring logic [User Query].
1.4. Document StructureThis document proceeds as follows: Section 2 details the approach to data acquisition and preparation. Section 3 establishes the framework for defining distinct wallet behavior profiles based on transaction data. Section 4 outlines the comprehensive feature engineering strategy. Section 5 evaluates potential modeling approaches and justifies the recommended path. Section 6 describes the design of the final 0-100 scoring system. Section 7 presents a step-by-step implementation plan and considers optimizations. Section 8 details the evaluation and validation strategy, emphasizing qualitative analysis. Finally, Section 9 provides concluding remarks.2. Data Acquisition and Preparation2.1. Data Source and SelectionThe foundational data for this project consists of raw, transaction-level logs from the Compound V2 protocol, provided via a Google Drive folder [User Query]. To ensure the analysis is based on a substantial and representative portion of the protocol's activity, the plan mandates selecting the three files with the largest sizes from this dataset [User Query]. This selection heuristic is based on the premise that larger files likely correspond to periods of higher transaction volume or cover more extended timeframes. Analyzing such periods is crucial as they potentially encompass diverse user behaviors and protocol conditions, including times of market stress, volatility, or significant liquidation events.1 Periods of high activity, whether driven by normal usage or market turmoil like cascading liquidations 3, offer richer data for distinguishing between robust, responsible behavior and fragile, risky strategies under pressure. Therefore, prioritizing the largest files increases the likelihood of capturing these informative data segments essential for building a comprehensive behavioral model.2.2. Understanding Data Structure via Subgraph SchemaThe provided data is described as "raw, transaction-level". While the exact format (e.g., CSV, JSON logs) needs confirmation upon inspection, it is anticipated that the records will reflect the events emitted by the Compound V2 smart contracts. The structure and content of these events are well-defined and publicly accessible, often indexed and queryable via The Graph protocol using subgraphs.5 The Compound V2 subgraph schema (schema.graphql) and manifest (subgraph.yaml) serve as invaluable references for understanding the expected data structure, even if the raw data requires more direct parsing.7Key events critical for behavioral analysis, expected to be present or derivable from the logs, include:
Mint: User supplies underlying assets, receives cTokens.7
Redeem: User burns cTokens, withdraws underlying assets.7
Borrow: User borrows underlying assets against collateral.7
RepayBorrow: User repays borrowed underlying assets.7
LiquidateBorrow: A liquidator repays a borrower's debt and seizes collateral.7
Within the records corresponding to these events, essential fields are expected, such as: timestamp, blockNumber, transactionHash, relevant wallet addresses (e.g., minter, redeemer, borrower, payer, liquidator), the cToken address identifying the specific market, the amount of the underlying asset involved, and potentially the corresponding cTokenAmount for Mint and Redeem events.7 It is important to note that raw event logs might be less structured than the processed entities served by a GraphQL endpoint.5 The raw data may necessitate direct parsing of Ethereum event logs, requiring identification of the relevant Compound V2 contract addresses (CTokens, Comptroller 7), filtering logs based on specific event signatures (topic hashes), and decoding the data payload according to the contract Application Binary Interfaces (ABIs).2.3. Data Cleaning and Preprocessing PipelineA robust preprocessing pipeline is essential to transform the raw data into a usable format for feature engineering. The planned steps include:
Loading: Implement efficient loading mechanisms capable of handling potentially large file sizes (multi-gigabyte). The choice of tools will depend on the confirmed data format (e.g., Pandas for CSV/JSON, potentially Dask or Polars for larger-than-memory datasets).
Parsing: Develop logic to accurately extract the critical fields identified (timestamps, addresses, event types, amounts, contract addresses) from each raw record. This step must accommodate the specific structure of the logs and handle event data decoding based on ABIs if necessary.
Type Conversion: Ensure all extracted data is converted to appropriate types. Timestamps should be converted to standard datetime objects (preferably UTC). Addresses should be standardized (e.g., lowercase checksummed format). Numerical values, particularly token amounts which can be very large (often represented in wei or with varying decimals), must be handled with libraries supporting high precision (e.g., Python's Decimal or equivalent BigInt types) to avoid floating-point errors. Compound protocol math often uses mantissas scaled by 1×1018.12
Timestamp Handling: Convert blockchain block timestamps into human-readable date/time formats. Verify consistency and assume UTC unless evidence suggests otherwise.
Address Standardization: Ensure all Ethereum addresses are consistently formatted (e.g., lowercase) to facilitate correct grouping and analysis by wallet.
Handling Missing/Anomalous Data: Blockchain data is generally immutable, making truly "missing" data rare. However, preprocessing should include checks for potential parsing errors, corrupted records, or unexpected data formats (e.g., transactions with illogical zero amounts, events missing expected fields). A strategy for logging or excluding such anomalies will be defined based on initial data exploration.
Data Aggregation (Initial): Group the cleaned transaction records by the primary wallet address associated with the action (e.g., minter, borrower, liquidator). This creates a chronological event history for each wallet, forming the basis for feature engineering.
A significant consideration during preprocessing is the potential absence of explicit protocol state information (like totalBorrows, totalSupply, exchangeRate, borrowIndex) within the raw transaction logs themselves.7 These state variables are crucial for calculating key risk metrics like historical Account Liquidity.12 While some subgraph implementations might calculate and store related fields 8, raw logs typically only capture the event occurrence. If these state snapshots are not directly available in the provided data, reconstructing them accurately for each historical transaction block would be necessary. This reconstruction could involve simulating state changes based on the sequence of events or potentially using external data sources like historical price oracle data (though the use of external data is restricted by the project constraints [User Query]). This potential need for historical state reconstruction represents a non-trivial challenge that must be addressed during implementation, possibly requiring approximation methods if full reconstruction proves infeasible within the project constraints.3. Defining Wallet Behavior Profiles3.1. Framework for DefinitionThe core task requires differentiating between "good" (reliable, responsible) and "bad" (risky, bot-like, exploitative) wallet behavior within Compound V2. Given the absence of external labels, these definitions must be constructed intrinsically, based on observable transaction patterns interpreted through the lens of DeFi lending principles, documented risks, and the specific mechanics of the Compound protocol. The goal set by Zeru Finance is to identify wallets whose behavior contributes positively or negatively to the health and stability of the protocol. This framework moves beyond a simple binary classification, aiming to place wallets on a spectrum of behavioral quality.3.2. Criteria for Responsible ("Good") UsageResponsible usage is characterized by actions that demonstrate prudent risk management, long-term engagement, and contribution to the protocol's core functions (lending and borrowing). Key indicators include:
Consistent Long-Term Participation: Wallets demonstrating sustained interaction over extended periods, suggesting genuine use for lending or borrowing rather than purely short-term speculation.15 This contrasts with hit-and-run strategies.
Healthy Collateralization Maintenance: Consistently maintaining borrowing positions with collateralization ratios significantly above the minimum required thresholds specified by the protocol's collateral factors.12 This involves actively managing positions to avoid nearing liquidation levels, perhaps aligning with risk profiles suggested for active or occasional DeFi users.16
Prudent Leverage Management: Borrowing amounts that represent a sensible fraction of the supplied collateral value, avoiding the maximization of leverage up to the collateral factor limit.12 This indicates an understanding and respect for volatility risk.1
Timely and Consistent Repayments: Regularly servicing debt, potentially making repayments that exceed the minimum accrued interest, demonstrating financial capacity and intent to repay.
Market Diversification (Potential Indicator): Engaging with multiple cToken markets (supplying various assets, borrowing different ones) could suggest a more sophisticated user applying portfolio diversification principles, potentially indicating a lower overall risk profile compared to concentrating risk in a single asset.17
Primary Focus on Supplying: Wallets that predominantly supply liquidity to the protocol with minimal or no borrowing inherently pose less risk to the protocol's solvency.18
3.3. Criteria for Risky & Exploitative ("Bad") BehaviorRisky or potentially exploitative behavior encompasses actions that demonstrate poor risk management, prioritize short-term gains over stability, potentially indicate automated strategies indifferent to risk, or actively attempt to manipulate protocol mechanics. Key indicators include:
High Liquidation Frequency: Accounts that are frequently liquidated or have numerous near-liquidation events signal poor position management, excessive risk-taking, or potentially automated strategies unable to react effectively to market volatility.4
Aggressive Leverage / Low Collateralization: Persistently operating with loan-to-value ratios extremely close to the liquidation threshold, maximizing leverage and minimizing safety margins.16 This is particularly risky given crypto asset volatility and the reliance on over-collateralization as the primary risk mitigation tool in anonymous DeFi lending.19
Large Borrows Preceding Collateral Devaluation: Patterns where significant borrowing occurs shortly before the value of the pledged collateral experiences a sharp decline. While difficult to confirm intent without price feeds, timing relative to known market downturns could be a flag for opportunistic or poorly timed leverage.1
Potential Oracle Exploit Patterns: Although difficult to detect definitively without access to the oracle price feeds used by the protocol 12, certain transaction sequences might raise suspicion. For example, rapidly supplying large amounts of a typically illiquid or volatile asset, borrowing a maximum amount of stable assets against it, and then effectively abandoning the position if the collateral price corrects might suggest attempts to exploit oracle latency or inaccuracies.3
Flash Loan-Like Activity (Sequential): While true flash loans occur atomically within a single transaction 1, observing patterns of very large, short-duration borrowing and repayment cycles spanning multiple transactions might indicate arbitrage, liquidation hunting, or other exploitative strategies that mimic flash loan use cases but operate sequentially.
High-Frequency / Repetitive / Low-Value Transactions: Transaction patterns strongly indicative of bot activity, such as numerous small, repetitive actions, rapid-fire transactions during liquidation events (potentially liquidation bots 4), or transactions appearing to probe protocol state rather than execute meaningful financial operations.
Extreme COMP Token Farming Behavior: Actions primarily driven by maximizing the acquisition of COMP governance tokens, which were distributed to suppliers and borrowers in V2.22 This could involve strategies like borrowing assets solely to re-supply them, recursive borrowing, or concentrating activity in markets with the highest COMP rewards, potentially regardless of underlying economic sense or associated risks. While COMP distribution was a legitimate protocol mechanism, behavior solely optimizing for it, especially if involving high leverage, could be flagged. Note that mechanisms were patched to prevent some manipulation 24, but incentive-driven behavior remains relevant.
Rapid Cycling / Wash Trading: High volumes of supply/withdraw or borrow/repay actions with little net change in the wallet's overall position. This could be aimed at artificially inflating activity metrics, generating COMP rewards, or potentially manipulating market interest rates if done at sufficient scale. This relates to broader concerns about manipulative trading activities.3
Association with Bad Debt: Wallets whose borrowing positions ultimately contribute to protocol bad debt, either because the collateral value falls below the debt value faster than liquidation can occur (Type I bad debt), or because the position becomes economically irrational for the borrower to close due to high transaction fees relative to remaining equity (Type II bad debt).4
3.4. Linking Behavior to Protocol HealthThe scoring system should ultimately reflect a wallet's impact on the overall health and stability of the Compound V2 protocol. Responsible users, as defined above, contribute positively by providing liquidity, utilizing the borrowing function predictably, and managing their risk effectively. Risky users, conversely, increase the protocol's systemic risk. Frequent liquidations strain the liquidation mechanism and can contribute to cascading price effects.3 Excessive leverage increases the potential for bad debt should liquidations fail or be insufficient.4 Exploitative users actively attempt to extract value in ways that can harm other users or the protocol's solvency (e.g., oracle manipulation 3).It is crucial to recognize that these behavioral categories exist on a spectrum. A single action (e.g., borrowing) is not inherently good or bad; context and pattern are key. The influence of protocol incentives like COMP distribution 22 further complicates the picture, as high activity might be incentive-driven rather than reflecting fundamental economic need. The definitions above provide a framework, but their operationalization relies heavily on the subsequent feature engineering step, which must quantify these behavioral indicators from the transaction data. The final score will represent a wallet's position along this multi-faceted behavioral spectrum.4. Feature Engineering Strategy4.1. GoalThe primary objective of the feature engineering phase is to translate the raw, time-ordered transaction logs for each wallet into a fixed-size vector of numerical features. This vector must comprehensively capture the diverse aspects of a wallet's interaction history with Compound V2, enabling the subsequent modeling stage to effectively differentiate between the behavioral profiles outlined in Section 3. The quality and creativity of feature engineering are paramount in this unsupervised context [User Query].4.2. Feature CategoriesFeatures will be designed across several categories to provide a holistic view of wallet behavior:A. Transactional Activity: Basic measures of interaction volume and type.
Counts: Total number of transactions; counts for each distinct event type (Mint, Redeem, Borrow, RepayBorrow, LiquidateBorrow initiated, LiquidateBorrow experienced).
Volume: Total value transacted per event type (requires approximation or use of token amounts if USD values are unavailable); average value per transaction type.
Frequency: Transaction rate over time (e.g., transactions per day/week/month); average and median time intervals between consecutive transactions; average time from a Borrow event to the corresponding RepayBorrow event(s).
Ratios: Ratio of borrow events to supply events (by count and/or volume); ratio of repayment volume to borrow volume; ratio of liquidations initiated to liquidations experienced.
B. Temporal Patterns: Measures capturing the timing and consistency of interactions.
Account Tenure: Time elapsed since the wallet's first recorded interaction; time elapsed since the most recent interaction.
Activity Consistency: Standard deviation of transaction frequency over time; length of the longest period of inactivity; measures of burstiness in activity.
Session Analysis (Approximate): Group transactions occurring within short time windows (e.g., minutes or hours) to estimate distinct user sessions. Analyze metrics within sessions, such as actions per session, session duration, and common action sequences.
C. Financial Health & Risk Metrics: Features quantifying the risk profile and financial status of the wallet within the protocol. Note: Calculation of these features may depend heavily on the ability to reconstruct or approximate historical protocol state (exchange rates, collateral factors, borrow balances) if not directly present in the raw data.
Historical Collateralization Ratio / Loan-to-Value (LTV): Metrics derived from tracking the estimated value of supplied collateral versus the value of outstanding debt over time. This requires applying appropriate collateral factors 12 and potentially using inferred or token-based value approximations. Features include: average LTV, minimum LTV, maximum LTV, standard deviation of LTV, time-weighted average LTV. Different LTV levels correspond to different user risk profiles.16
Time Spent Near Liquidation: Calculate the proportion of time the wallet's estimated LTV was within a critical threshold (e.g., 5-10%) of its liquidation limit. High values indicate persistent high-risk positioning.
Liquidation History: Number of times the wallet was liquidated as a borrower; total value liquidated (estimated); number of liquidations performed by the wallet as a liquidator; average size of liquidations performed/experienced.4
Interest Accrued/Paid Profile: Estimate the total interest earned on supplied assets versus total interest paid on borrowed assets over the wallet's history. This requires applying the relevant Compound V2 interest rate models 13 to tracked supply/borrow balances over time. Features could include total net interest, ratio of interest paid to principal borrowed, average borrow APY experienced. Consistently high interest payments relative to borrow size might indicate risky positions or high utilization borrowing.
Reserve Interaction: Track any direct or indirect interaction with protocol reserves, such as through the portion of seized collateral allocated to reserves during liquidations (seize share).12
D. Protocol Interaction & Potential Strategy Indicators: Features aimed at uncovering more subtle behavioral patterns or potential strategic motives.
Market Diversity: Number of distinct cToken markets the wallet has interacted with (supplied to or borrowed from). Higher diversity might correlate with more sophisticated users.17
COMP Interaction (Proxy): Estimate the amount of COMP tokens likely accrued by the wallet based on its supply/borrow activity in eligible markets during the V2 distribution period. The V2 distribution mechanism allocated COMP based on market activity and interest generated.23 Features could include estimated total COMP earned, ratio of COMP earned to total transaction value, concentration of activity in high-COMP-reward markets. This proxies potential "yield farming" behavior.22
Gas Price Behavior: Analyze the gas prices paid for transactions. Features include average gas price, median gas price, frequency of paying significantly above or below the prevailing network average (requires external context or comparison within the dataset). High gas prices can indicate urgency, potentially during liquidations or competitive actions.4 Unusually low gas prices might indicate non-time-sensitive bot activity.
Transaction Complexity: Measure patterns involving multiple different cTokens or actions within very short timeframes. This could indicate complex arbitrage strategies, collateral swaps, or sophisticated portfolio management.
4.3. Feature Calculation Considerations
Time Windows: Many features should be calculated over multiple time horizons (e.g., lifetime, last 90 days, last 30 days) to capture both long-term reputation and recent behavioral shifts.
Normalization/Scaling: Before inputting features into any model, appropriate scaling (e.g., Min-Max scaling to , Standardization to zero mean and unit variance) must be applied to ensure features with larger ranges do not disproportionately influence distance-based or gradient-based algorithms.
Handling Price Data: The constraint against using external data significantly impacts features requiring USD valuations (e.g., absolute volume, precise LTV). The plan must proceed assuming one of the following:

Possibility 1 (Less Likely): The raw data contains sufficient information (e.g., embedded oracle prices or exchange rates) to derive reliable historical USD values.
Possibility 2 (More Likely): Precise USD values cannot be reliably determined. In this case, features will be adapted:

Volumes will be measured in the underlying token amounts.
LTV and collateralization metrics will be calculated based on token quantities and the cToken exchange rates (which reflect relative value within Compound 10), and collateral factors.12 While the Comptroller uses an external Price Oracle for its internal calculations 12, accessing its historical state might be infeasible. Features will therefore reflect risk relative to the protocol's internal accounting rather than absolute external market values. This is a critical implementation detail requiring careful handling.




4.4. Proposed Table: Feature Engineering SummaryTo provide a clear overview of the engineered features, their rationale, and connection to observed behaviors, the following table structure is proposed:Table 1: Feature Engineering Summary
Feature CategoryFeature NameDescription & Calculation LogicRationale / Behavioral LinkSupporting Concepts/RisksTransactionaltx_count_totalTotal number of interactions (Mint, Redeem, Borrow, Repay, Liquidate).Basic activity level.-borrow_to_supply_ratio_volRatio of total borrow volume (token-based) to total supply volume (token-based).High ratio indicates leverage preference.18repay_ratioRatio of total repayment volume to total borrow volume.Values < 1 indicate outstanding debt; significantly > 1 might indicate rapid cycling.-Temporalaccount_age_daysDays since first transaction.Longer tenure might indicate more experience or commitment.17days_since_last_txDays since last transaction.High values indicate inactivity.-activity_consistency_stddevStandard deviation of daily/weekly transaction counts.High values indicate bursty, inconsistent activity.-Financial Health/Riskavg_ltv_90dAverage estimated Loan-to-Value over the last 90 days (calculated using token values, exchange rates, collateral factors).High average LTV indicates sustained high risk.12min_ltv_alltimeMinimum estimated LTV reached over the wallet's lifetime.Very low minimums indicate periods close to liquidation.2liquidation_count_borrowerNumber of times the wallet's positions were liquidated.Direct measure of poor risk management or extreme strategy failure.4liquidation_count_liquidatorNumber of times the wallet acted as a liquidator.Indicates participation in liquidation process; potentially sophisticated or bot activity.4time_near_liquidation_pctPercentage of active borrowing time spent with LTV near the liquidation threshold.High percentage indicates persistent risky behavior.16Protocol Interactionmarket_diversity_countNumber of unique cToken markets interacted with.Low diversity might indicate concentrated risk; high diversity could indicate sophistication.17comp_earned_estimate_ratioEstimated COMP earned (based on V2 rules) as a ratio of total transaction volume.High ratio might indicate behavior heavily driven by COMP farming incentives.22avg_gas_price_relativeAverage gas price paid relative to dataset average (or block averages if calculable).High relative gas may indicate urgency (e.g., liquidations); low may indicate bots.4complex_action_freqFrequency of performing multiple distinct actions (supply, borrow, repay) across different markets within a short time window.May indicate complex strategies (arbitrage, collateral swaps).-
This table serves as a crucial reference, structuring the diverse features derived from the transaction data. It explicitly connects each engineered metric to its calculation, its intended interpretation in terms of wallet behavior, and the underlying DeFi concepts or documented risks that motivate its inclusion. This structured approach enhances the clarity, reproducibility, and justification of the feature set, aligning with the evaluation criteria emphasizing methodology quality [User Query].5. Modeling Approach and Justification5.1. Addressing the No-Label ConstraintThe fundamental constraint of this project – the absence of pre-defined labels for wallet creditworthiness or risk – dictates the choice of modeling methodology. Standard supervised learning techniques (classification or regression) cannot be directly applied as there is no target variable to predict. Therefore, the approach must rely on unsupervised learning methods, heuristic rule-based systems, or a combination thereof to infer behavioral quality from the engineered features.5.2. Evaluation of Unsupervised MethodsSeveral unsupervised techniques could potentially be applied:
Clustering: Algorithms like K-Means, DBSCAN, or Hierarchical Clustering group wallets based on similarities in their feature vectors.

Potential Application: Could automatically identify distinct behavioral archetypes present in the data, such as 'passive long-term suppliers', 'active short-term borrowers', 'frequent liquidators', or 'COMP farmers'. These archetypes might align with different risk profiles.16
Limitations: Requires careful feature selection and scaling, as cluster quality is sensitive to input data. Determining the optimal number of clusters can be subjective (requiring methods like elbow analysis or silhouette scores). Crucially, the resulting clusters need interpretation to map them onto a meaningful risk scale (0-100), which remains a subjective step. High-dimensional feature spaces can also pose challenges ('curse of dimensionality').


Anomaly Detection: Techniques like Isolation Forest, One-Class SVM, or Autoencoders aim to identify data points that deviate significantly from the norm.

Potential Application: Could effectively flag wallets exhibiting highly unusual or outlier behavior. This might correspond to risky actions (e.g., extreme leverage just before liquidation 16), bot-like activity (e.g., very high frequency, repetitive patterns 4), or potentially exploitative strategies designed to game the protocol (e.g., patterns mimicking oracle manipulation attempts 3, or historical flash loan patterns 20). The underlying assumption is that "normal" behavior, exhibited by the majority, is generally less risky.
Limitations: Highly sensitive to the definition of "normality" captured by the features. Novel but legitimate strategies might be incorrectly flagged as anomalous. Performance heavily depends on the feature set's ability to capture relevant deviations. Anomaly scores typically indicate how anomalous a point is, but don't provide a nuanced ranking across the entire spectrum of "normal" behavior.


5.3. Evaluation of Heuristic / Rule-Based ScoringThis approach involves defining a set of rules and weights based on domain knowledge to calculate a score directly from the features.
Potential Application: Rules can directly encode DeFi best practices and known risks. For example: assign negative points for each liquidation event 4, add points for maintaining a high average collateralization ratio 16, penalize high LTV volatility 2, reward longer account tenure.17
Advantages: High interpretability – the contribution of each feature/rule to the final score is explicit. Allows direct incorporation of expert knowledge about Compound V2 mechanics and risks.
Limitations: Designing a comprehensive and well-balanced set of rules can be complex. Assigning appropriate weights and thresholds can be subjective without validation data. Simple rule sets might not capture complex interactions between features and could potentially be "gamed" by users who understand the scoring logic. However, the requirement is for a non-trivial system [User Query], implying a sophisticated heuristic could meet the criteria.
5.4. Evaluation of Hybrid ModelsHybrid approaches seek to combine the strengths of the methods above.
Potential Application: Use clustering to identify broad user segments, then apply different heuristic rules or anomaly detection sensitivity within each segment. Alternatively, use anomaly scores generated by an anomaly detection model as input features into a heuristic scoring system (e.g., assigning a significant negative weight to a high anomaly score).
Advantages: Can potentially achieve greater robustness and nuance by leveraging both data-driven pattern discovery (clustering/anomaly detection) and expert-driven logic (heuristics).
Limitations: Increased complexity in implementation, interpretation, and calibration.
5.5. Recommended Approach and JustificationBased on the project goals and constraints, the recommended approach is a Hybrid Model, centered around a sophisticated, weighted Heuristic Scoring System, augmented by inputs from Anomaly Detection.
Justification:

Interpretability and Explainability: The heuristic core provides direct traceability from features (representing specific behaviors) to the final score. This is crucial for a system intended to assess reliability and risk, meeting a key evaluation criterion [User Query]. Rules can be explicitly linked to documented DeFi risks and best practices.1
Non-Triviality: A well-designed heuristic system incorporating a large number of weighted features (from Section 4), potentially with non-linear interactions or conditional logic, can achieve the required complexity and be difficult to reverse-engineer or game easily [User Query].
Leveraging Data Patterns: Integrating anomaly scores allows the system to account for statistically unusual behavior patterns that might represent novel risks, bot activity, or exploits not explicitly captured by the predefined heuristic rules.3 This adds a data-driven safety net.
Domain Knowledge Integration: The heuristic framework provides a natural way to embed knowledge about Compound V2 mechanics (e.g., collateral factors 12, liquidation logic 10) and general DeFi risks (e.g., volatility impact 2).
Adaptability and Calibration: While lacking quantitative labels, the weights within the heuristic model can be iteratively tuned based on qualitative validation (detailed wallet analysis, Section 8.2) and expert judgment regarding the relative severity of different behaviors (e.g., liquidations vs. low market diversity).
Role of Clustering: Clustering can serve as a valuable offline tool for exploratory data analysis. It can help understand the natural groupings of wallets based on behavior and validate whether the final heuristic scores effectively differentiate between these identified segments. However, due to the challenges in directly mapping clusters to a continuous 0-100 score and interpreting cluster meanings objectively, it is not recommended as the primary scoring mechanism itself.


This hybrid approach pragmatically addresses the no-label constraint by combining interpretable, knowledge-driven rules with data-driven detection of unusual patterns, aiming for a robust, explainable, and non-trivial scoring system. The success hinges significantly on the quality of the preceding feature engineering step.6. Scoring System Design6.1. Mapping Model Output to 0-100 ScoreThe chosen hybrid model (heuristic core + anomaly detection input) will produce a raw numerical score for each wallet. This raw score needs to be transformed onto the required 0-100 scale, where 100 represents the most responsible/reliable behavior and 0 represents the most risky/exploitative behavior. Several methods can achieve this transformation:
Min-Max Scaling: This linearly maps the raw scores based on the minimum and maximum raw scores observed across the entire dataset:
Scorescaled​=100×Scoremax​−Scoremin​Scoreraw​−Scoremin​​ (adjusted if lower raw score means better behavior). While simple, this method is highly sensitive to outliers, as a single extreme raw score can compress the range for all other wallets.
Percentile Ranking: This method assigns a score based on the wallet's rank within the distribution of raw scores. For example, a wallet with a raw score at the 95th percentile (assuming higher raw score is better) could be assigned a score of 95. This ensures a uniform distribution of scores across the population and is robust to outliers.
Sigmoid Transformation: A sigmoid function (e.g., the logistic function) can map raw scores to the  range, which can then be scaled to 0-100. Scorescaled​=100×1+e−k(Scoreraw​−x0​)1​. This allows for non-linear mapping, potentially providing better separation at the extremes of the score range. Calibration of the parameters k (steepness) and x0​ (midpoint) is required.
Recommendation: Utilize Percentile Ranking or a carefully calibrated Sigmoid Transformation. Percentile ranking offers robustness and guarantees a full spread of scores relative to the population. A sigmoid function, if calibrated well based on the observed distribution of raw scores and qualitative analysis, can offer more control over the score sensitivity in different parts of the range. Min-Max scaling is generally discouraged due to its outlier sensitivity.6.2. Score InterpretationClear interpretation guidelines are essential for the score's utility. The 0-100 scale should correspond to distinct behavioral profiles:
High Scores (e.g., 80-100): Indicate wallets exhibiting consistently responsible behavior. Characteristics likely include long tenure, maintaining significant collateral buffers (low LTV) 16, infrequent or no liquidations 4, potentially diverse market participation 17, and timely repayments.
Medium Scores (e.g., 40-79): Represent wallets with mixed behavioral signals. This could include newer users still establishing a history, users employing moderate leverage, occasional proximity to liquidation thresholds, or perhaps focused COMP farming without excessive leverage.22
Low Scores (e.g., 0-39): Signal wallets exhibiting high-risk or potentially problematic behavior. Characteristics likely include frequent liquidations 4, consistently high LTV 16, association with bad debt 4, patterns indicative of aggressive bot activity 4, or behavior flagged by anomaly detection as highly unusual/suspicious.3
The scoring system must be designed such that the score is monotonically related to perceived risk – a lower score must always imply equal or greater assessed risk.6.3. Calibration and WeightingThe core of the heuristic model lies in the assignment of weights to the various input features (Section 4) and the potential contribution from anomaly scores. Since supervised training is not possible, this calibration process will be iterative and rely heavily on domain expertise and qualitative validation:
Initial Weighting: Assign initial weights based on the perceived importance and severity of different behaviors according to DeFi principles and documented risks. For example, features related to liquidations 4 or operating extremely close to liquidation thresholds 16 should likely receive significant negative weights. Features representing longevity or diversification 17 might receive positive weights. Anomaly scores would receive negative weights.
Sensitivity Analysis: Analyze how changes in individual feature weights or anomaly score contributions affect the overall score distribution and the ranking of wallets. This helps understand the influence of each component.
Qualitative Validation Feedback: Use the findings from the wallet analysis (Section 8.2) to refine weights. If high-scoring wallets exhibit some undesirable patterns, or low-scoring wallets appear relatively benign upon manual review, the weights contributing to those scores need adjustment.
Iteration: Repeat steps 2 and 3 until the scores align well with the behavioral definitions (Section 3) and expert judgment across a sample of wallets.
This iterative, qualitative calibration process is crucial for ensuring the score is meaningful and robust. The final set of weights and rules should be sufficiently complex (non-trivial) to capture nuanced behaviors and resist simple gaming, fulfilling a key project constraint [User Query]. The emphasis is on creating a system that reflects a deep understanding of Compound V2 interactions, rather than a simple checklist.7. Implementation Plan: Steps and Features7.1. Step-by-Step WorkflowThe implementation will follow a logical sequence from data ingestion to final score output:
Data Ingestion: Load the three selected largest raw data files into the processing environment.
Preprocessing: Execute the cleaning, parsing, type conversion, and standardization pipeline defined in Section 2.3.
Wallet Aggregation: Group processed transactions by wallet address, creating time-ordered histories for each unique user.
Feature Engineering: Calculate the comprehensive set of features detailed in Section 4 for every wallet. This step includes handling different time windows and addressing the potential need for historical state reconstruction or approximation.
Model Training/Application:

If incorporating anomaly detection: Train the chosen anomaly detection model(s) (e.g., Isolation Forest) on the engineered feature set. Generate anomaly scores for each wallet.
Apply the defined heuristic rules, incorporating the assigned weights and potentially the anomaly scores as inputs.
Combine outputs according to the hybrid model design to produce a raw score per wallet.


Scoring: Apply the selected scaling method (e.g., Percentile Ranking or Sigmoid Transformation, Section 6.1) to convert raw scores into the final 0-100 scale.
Output Generation: Produce the required CSV file containing wallet_address and score for the top 1,000 wallets, sorted in descending order of score [User Query].
Wallet Analysis Preparation: Select representative samples of high-scoring (e.g., top 5-10) and low-scoring (e.g., bottom 5-10) wallets for the qualitative analysis phase (Section 8.2).
7.2. Key Features/Modules of Implementation (Code Structure Plan)To ensure modularity, maintainability, and testability, the implementation code should be structured into distinct modules:
data_loader.py: Responsible for reading the raw data files (handling different potential formats) and performing initial parsing into a structured intermediate representation.
preprocessor.py: Contains functions for data cleaning (type conversion, address standardization, timestamp handling, anomaly logging) as outlined in Section 2.3.
feature_engineer.py: The core module implementing the calculation logic for all features defined in Section 4 and Table 1. This will likely be the most complex module, potentially containing sub-modules or helper functions, especially if state reconstruction is required. It will take the preprocessed, wallet-aggregated data as input and output a feature matrix (wallets x features).
modeling/: A directory containing the implementation of the chosen modeling components.

anomaly_detector.py: (If used) Contains the code for training and predicting with the anomaly detection model(s).
heuristic_scorer.py: Implements the logic for applying the weighted heuristic rules to the feature matrix, producing raw scores.


scoring.py: Contains functions for applying the final 0-100 scaling transformation (e.g., percentile calculation, sigmoid function) to the raw scores.
main.py: The main script that orchestrates the entire pipeline, calling functions from the other modules in the correct sequence (Load -> Preprocess -> Feature Engineer -> Model -> Score -> Output).
config.yaml (or similar): A configuration file to store parameters like input/output file paths, feature engineering settings (e.g., time windows), model parameters (e.g., anomaly detection settings), heuristic weights, and scoring thresholds. This promotes flexibility and avoids hardcoding values.
7.3. Potential OptimizationsDuring development and for potential future production deployment, several optimizations can be considered:
Computational Efficiency:

Utilize efficient data manipulation libraries designed for large datasets (e.g., Pandas with optimized types, Dask for parallel/out-of-core processing, or Polars known for performance).
Parallelize computationally intensive tasks, particularly feature engineering calculations that can often be performed independently per wallet or per feature.
Optimize any state reconstruction logic if implemented, as this can be computationally expensive.


Feature Selection: If the initial set of engineered features becomes very large (>100s), dimensionality reduction techniques could be explored after initial analysis. Methods like removing low-variance features, eliminating highly correlated features, or using feature importance scores derived from the anomaly detection model (if applicable) can help reduce noise, potentially improve model robustness, and decrease computation time. However, care must be taken not to remove features critical for interpretability in the heuristic model.
Sampling: For faster iteration during the development and debugging phases, work with smaller, representative samples of the data before running the full pipeline on the entire selected dataset.
8. Evaluation and Validation Strategy8.1. Assessing Score Consistency and RobustnessIn the absence of ground truth labels, evaluating the scoring system relies on assessing its internal consistency, stability, and alignment with expected behaviors under different conditions:
Score Distribution Analysis: Examine the statistical distribution of the final 0-100 scores across all wallets. Is it roughly uniform (as expected from percentile ranking), normally distributed, or heavily skewed? Does it utilize the full 0-100 range? Unexpected distributions might indicate issues with feature scaling, heuristic weighting, or the final transformation.
Sensitivity Analysis: Evaluate how sensitive the scores are to minor perturbations in the input data or small changes in the heuristic weights/model parameters. A robust system should exhibit stable scores; large fluctuations from small changes indicate potential instability.
Backtesting (Conceptual): Leverage the historical nature of the data. If the dataset covers significant market events (e.g., major price crashes like the one mentioned in March 2020 2, periods of high volatility 3, or large liquidation cascades 4), analyze the scores assigned to wallets before these events. Did wallets that were subsequently liquidated or exhibited distress during the event have lower scores beforehand? While not definitive proof, correlation between lower scores and subsequent negative outcomes would support the score's validity as a risk indicator.
Parameter Tuning Evaluation: Systematically vary key parameters (e.g., thresholds in heuristic rules, weights assigned to major risk factors like liquidations, parameters of the anomaly detection model) and observe the impact on the score distribution and the ranking of specific, well-understood wallet archetypes (identified through manual inspection or clustering).
8.2. Qualitative Validation via Wallet AnalysisThis is the most critical validation step in an unsupervised setting. It involves deep dives into the transaction histories of selected wallets to qualitatively assess whether their assigned scores align with their observed behavior.
Process:

Selection: Identify a sample of N (e.g., 5 to 10) wallets with the highest scores and N wallets with the lowest scores from the generated output. Additionally, selecting a few wallets from the middle range can provide further insight.
Detailed Review: For each selected wallet, meticulously examine its complete transaction history within the dataset. Reconstruct its journey within Compound V2: when did it join? What assets did it supply/borrow? How did its LTV evolve? Was it ever liquidated? Did it liquidate others? How frequently did it interact? Are there patterns suggesting bot activity or specific strategies (e.g., COMP farming)?
Comparison: Compare the observed behavioral patterns against the criteria established for "good" and "bad" behavior in Section 3.
Assessment: Judge whether the assigned score (high, medium, or low) accurately reflects the observed behavioral profile.
Documentation: Document the findings for each wallet, providing specific examples from their transaction history to justify the assessment. Highlight any discrepancies where the score seems misaligned with the observed behavior and hypothesize potential reasons (e.g., a missing feature, incorrect weighting, an edge case).


Deliverable: As required [User Query], a concise (e.g., one-page) analysis summarizing these findings will be produced. This document will present the patterns observed in high and low-scoring wallets and provide justification for why their scores are appropriate (or identify areas for refinement). This qualitative feedback loop is essential for building confidence in the scoring system and iteratively improving its accuracy and relevance. This directly addresses the evaluation criterion regarding the "Insightfulness of wallet behavior analysis" [User Query]. This manual check provides the necessary grounding that purely quantitative metrics cannot offer in this unsupervised scenario.
8.3. Framework for Monitoring and IterationThe development of this scoring system should not be viewed as a one-time task. A framework for ongoing monitoring and iteration is recommended:
Score Monitoring: Regularly monitor the distribution of scores as new data becomes available (if the system is deployed operationally). Shifts in the distribution could indicate changes in overall user behavior within Compound V2 or potential issues with the scoring model.
Periodic Re-evaluation: Schedule periodic reviews (e.g., quarterly, semi-annually) to re-evaluate the feature set, model weights, and overall methodology. As the DeFi landscape evolves, user strategies change, and new risks may emerge, requiring updates to the scoring logic.
Feedback Loop: Establish a formal process for incorporating feedback, particularly insights gained from ongoing qualitative wallet analysis or from the eventual use of the scores by Zeru Finance, back into the model development cycle. This ensures the scoring system remains relevant and accurate over time.
9. Conclusion9.1. Summary of MethodologyThis document has outlined a detailed methodology for developing a wallet scoring system for Compound V2, addressing the specific requirements and constraints set forth by Zeru Finance. The proposed approach begins with careful selection and preparation of the raw transaction data, leveraging insights from the protocol's structure and potential challenges like historical state reconstruction. It establishes clear, data-driven definitions for responsible versus risky behavior based on DeFi principles. A comprehensive feature engineering strategy is detailed, designed to capture diverse transactional, temporal, financial health, and protocol interaction patterns. Recognizing the absence of labels, a hybrid modeling approach is recommended, combining the interpretability of a weighted heuristic system with the pattern-detection capabilities of anomaly detection. The plan includes designing the final 0-100 score mapping and emphasizes an iterative calibration process heavily reliant on qualitative validation through in-depth wallet analysis. Finally, a structured implementation plan and a robust evaluation strategy are presented.9.2. Expected ValueThe execution of this methodology is expected to yield a nuanced, explainable, and non-trivial credit scoring system for Compound V2 wallets, tailored to Zeru Finance's objectives. By grounding the definitions of behavior in protocol mechanics and DeFi risks 1 and operationalizing these through careful feature engineering and a hybrid modeling approach, the system aims to effectively differentiate between reliable users and those posing higher risks, despite the lack of external labels. The emphasis on qualitative validation ensures the scores are meaningful and aligned with observable on-chain behavior. The resulting system will provide a valuable tool for assessing wallet interactions within Compound V2 based solely on their historical transaction data.9.3. Next StepsWith this methodology defined, the next phase involves the implementation of the outlined steps: acquiring and processing the data, engineering the features, building and calibrating the hybrid model, generating scores, and performing the crucial qualitative wallet analysis for validation and refinement. This plan provides a clear roadmap for developing the required scoring system.